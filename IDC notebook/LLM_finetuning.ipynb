{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d104c17-c98c-4e4a-89d8-1b979bde8f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of NVIDIA GPUs available on the system:\n",
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!echo \"List of NVIDIA GPUs available on the system:\"\n",
    "!nvidia-smi --query-gpu=gpu_name --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0481ac0-2bde-470d-9ebf-5ac311b26a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GPU used by this notebook:\n",
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!echo \"NVIDIA GPU used by this notebook:\"\n",
    "!nvidia-smi --query-gpu=name --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6688406c-6b5a-44fc-9d13-4cd77b97e234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement bigdl-llm==2.4.0b20231116 (from versions: 2.4.0b20230930, 2.4.0, 2.5.0b0, 2.5.0b1, 2.5.0b2, 2.5.0b20231201, 2.5.0b20231202, 2.5.0b20231203, 2.5.0b20231204, 2.5.0b20231205, 2.5.0b20231206, 2.5.0b20231207, 2.5.0b20231208, 2.5.0b20231209, 2.5.0b20231210, 2.5.0b20231211, 2.5.0b20231212, 2.5.0b20231213, 2.5.0b20231214, 2.5.0b20231215, 2.5.0b20231216, 2.5.0b20231217, 2.5.0b20231218, 2.5.0b20231219, 2.5.0b20231220, 2.5.0b20231221, 2.5.0b20231222, 2.5.0b20231223, 2.5.0b20231224, 2.5.0b20231225, 2.5.0b20231226, 2.5.0b20231227, 2.5.0b20231228, 2.5.0b20231229, 2.5.0b20231230, 2.5.0b20231231, 2.5.0b20240101, 2.5.0b20240103, 2.5.0b20240104, 2.5.0b20240105, 2.5.0b20240106, 2.5.0b20240107, 2.5.0b20240108, 2.5.0b20240109, 2.5.0b20240110, 2.5.0b20240111, 2.5.0b20240112, 2.5.0b20240113, 2.5.0b20240114, 2.5.0b20240115, 2.5.0b20240116, 2.5.0b20240117, 2.5.0b20240118, 2.5.0b20240122, 2.5.0b20240123, 2.5.0b20240124, 2.5.0b20240125, 2.5.0b20240126, 2.5.0b20240127, 2.5.0b20240128, 2.5.0b20240129, 2.5.0b20240130, 2.5.0b20240201, 2.5.0b20240202, 2.5.0b20240203, 2.5.0b20240204, 2.5.0b20240205, 2.5.0b20240206, 2.5.0b20240207, 2.5.0b20240208, 2.5.0b20240209, 2.5.0b20240210, 2.5.0b20240211, 2.5.0b20240212, 2.5.0b20240213, 2.5.0b20240218, 2.5.0b20240219, 2.5.0b20240220, 2.5.0b20240221, 2.5.0b20240222, 2.5.0b20240223, 2.5.0b20240224, 2.5.0b20240225, 2.5.0b20240226, 2.5.0b20240227, 2.5.0b20240228, 2.5.0b20240229, 2.5.0b20240301, 2.5.0b20240302, 2.5.0b20240303, 2.5.0b20240304, 2.5.0b20240305, 2.5.0b20240306, 2.5.0b20240307, 2.5.0b20240308, 2.5.0b20240309, 2.5.0b20240310, 2.5.0b20240311, 2.5.0b20240312, 2.5.0b20240313, 2.5.0b20240314, 2.5.0b20240315, 2.5.0b20240316, 2.5.0b20240317, 2.5.0b20240318)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for bigdl-llm==2.4.0b20231116\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --pre 'bigdl-llm[xpu]==2.4.0b20231116' --no-warn-script-location -f https://developer.intel.com/ipex-whl-stable-xpu > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "218ed238-04be-4fe0-aeb5-942682a600d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation in progress, please wait...\n",
      "zsh:1: no matches found: bigdl-llm[xpu]==2.4.0b20231116\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "accelerate 0.23.0 requires torch>=1.10.0, which is not installed.\n",
      "peft 0.5.0 requires torch>=1.13.0, which is not installed.\n",
      "datasets 2.15.0 requires huggingface-hub>=0.18.0, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mInstallation completed.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import site\n",
    "from pathlib import Path\n",
    "\n",
    "!echo \"Installation in progress, please wait...\"\n",
    "!{sys.executable} -m pip cache purge > /dev/null\n",
    "!{sys.executable} -m pip install --pre bigdl-llm[xpu]==2.4.0b20231116 --no-warn-script-location -f https://developer.intel.com/ipex-whl-stable-xpu > /dev/null\n",
    "!{sys.executable} -m pip install peft==0.5.0 --no-deps > /dev/null\n",
    "!{sys.executable} -m pip install accelerate==0.23.0 --no-deps --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install  transformers==4.34.0 --no-warn-script-location > /dev/null \n",
    "!{sys.executable} -m pip install datasets==2.15 --no-warn-script-location > /dev/null 2>&1 \n",
    "!{sys.executable} -m pip install fsspec==2023.9.2 > /dev/null  2>&1\n",
    "!echo \"Installation completed.\"\n",
    "\n",
    "def get_python_version():\n",
    "    return \"python\" + \".\".join(map(str, sys.version_info[:2]))\n",
    "\n",
    "def set_local_bin_path():\n",
    "    local_bin = str(Path.home() / \".local\" / \"bin\") \n",
    "    local_site_packages = str(\n",
    "        Path.home() / \".local\" / \"lib\" / get_python_version() / \"site-packages\"\n",
    "    )\n",
    "    sys.path.append(local_bin)\n",
    "    sys.path.insert(0, site.getusersitepackages())\n",
    "    sys.path.insert(0, sys.path.pop(sys.path.index(local_site_packages)))\n",
    "\n",
    "set_local_bin_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da13135b-8d88-481c-a71e-1d8bd5a0ac6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mERROR)\n\u001b[1;32m     24\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigdl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mERROR)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mintel_extension_for_pytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mipex\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from math import ceil\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, module=\"intel_extension_for_pytorch\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, module=\"torchvision.io.image\", lineno=13\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message=\"You are using the default legacy behaviour\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Parameter.*\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\"This implementation of AdamW is deprecated\",\n",
    ")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"28\"\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"bigdl\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from bigdl.llm.transformers.qlora import (\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training as prepare_model,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from bigdl.llm.transformers.qlora import PeftModel\n",
    "import transformers\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    LlamaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c900189-ff72-4193-97b2-6213bcf73406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def check_disk_space(path=\"~/.cache/huggingface/\"):\n",
    "    abs_path = os.path.expanduser(path)\n",
    "    total, used, free = shutil.disk_usage(abs_path)\n",
    "    print(f\"Total: {total // (2**30)} GiB\")\n",
    "    print(f\"Used: {used // (2**30)} GiB\")\n",
    "    print(f\"Free: {free // (2**30)} GiB\")\n",
    "\n",
    "# Example usage\n",
    "check_disk_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8de66-236e-43f0-804d-65f73d3a0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODELS = {\n",
    "    \"0\": \"NousResearch/Nous-Hermes-Llama-2-7b\",  # https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b\n",
    "    \"1\": \"NousResearch/Llama-2-7b-chat-hf\",  # https://huggingface.co/NousResearch/Llama-2-7b-chat-hf\n",
    "    \"2\": \"NousResearch/Llama-2-13b-hf\",  # https://huggingface.co/NousResearch/Llama-2-13b-hf\n",
    "    \"3\": \"NousResearch/CodeLlama-7b-hf\",  # https://huggingface.co/NousResearch/CodeLlama-7b-hf\n",
    "    \"4\": \"Phind/Phind-CodeLlama-34B-v2\",  # https://huggingface.co/Phind/Phind-CodeLlama-34B-v2\n",
    "    \"5\": \"openlm-research/open_llama_3b_v2\",  # https://huggingface.co/openlm-research/open_llama_3b_v2\n",
    "    \"6\": \"openlm-research/open_llama_13b\",  # https://huggingface.co/openlm-research/open_llama_13b\n",
    "    \"7\": \"HuggingFaceH4/zephyr-7b-beta\", # https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
    "}\n",
    "BASE_MODEL = BASE_MODELS[\"3\"]\n",
    "DATA_PATH = \"b-mc2/sql-create-context\"\n",
    "MODEL_PATH = \"./final_model\"\n",
    "ADAPTER_PATH = \"./lora_adapters\"\n",
    "DEVICE = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "LORA_CONFIG = LoraConfig(\n",
    "    r=16,  # rank\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "MODEL_CACHE_PATH = \"/home/common/data/Big_Data/GenAI/llm_models\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Final model will be saved to: {MODEL_PATH}\")\n",
    "print(f\"LoRA adapters will be saved to: {ADAPTER_PATH}\")\n",
    "print(f\"Finetuning Model: {BASE_MODEL}\")\n",
    "print(f\"Using dataset from: {DATA_PATH}\")\n",
    "print(f\"Model cache: {MODEL_CACHE_PATH}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb75fdd-2449-4c6e-b851-06bf7279e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_sql(input_question, context, output=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a prompt for fine-tuning the LLM model for text-to-SQL tasks.\n",
    "\n",
    "    Parameters:\n",
    "        input_question (str): The input text or question to be converted to SQL.\n",
    "        context (str): The schema or context in which the SQL query operates.\n",
    "        output (str, optional): The expected SQL query as the output.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string serving as the prompt for the fine-tuning task.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Input:\n",
    "{input_question}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39955f-ec69-4ac2-b216-96db2738493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(base_model_id: str):\n",
    "    \"\"\"Downloads / Loads the pre-trained model in NF4 datatype and tokenizer based on the given base model ID for training.\"\"\"\n",
    "    local_model_id = base_model_id.replace(\"/\", \"--\")\n",
    "    local_model_path = os.path.join(MODEL_CACHE_PATH, local_model_id)\n",
    "    print(f\"local model path is: {local_model_path}\")\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_model_path,\n",
    "            load_in_low_bit=\"nf4\",\n",
    "            optimize_model=False,\n",
    "            torch_dtype=torch.float16,\n",
    "            modules_to_not_convert=[\"lm_head\"],\n",
    "        )\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        sys.exit()\n",
    "        logging.info(\n",
    "            f\"Model not found locally. Downloading {base_model_id} to cache...\"\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id,\n",
    "            load_in_low_bit=\"nf4\",\n",
    "            optimize_model=False,\n",
    "            torch_dtype=torch.float16,\n",
    "            modules_to_not_convert=[\"lm_head\"],\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        if \"llama\" in base_model_id.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(local_model_path)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "    except OSError:\n",
    "        logging.info(\n",
    "            f\"Tokenizer not found locally. Downloading tokenizer for {base_model_id} to cache...\"\n",
    "        )\n",
    "        if \"llama\" in base_model_id.lower():\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(base_model_id)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9ef377c-cd87-4031-8afa-2929d547ad24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFineTuner\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A class to handle the fine-tuning of LLM models.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class FineTuner:\n",
    "    \"\"\"A class to handle the fine-tuning of LLM models.\"\"\"\n",
    "\n",
    "    def __init__(self, base_model_id: str, model_path: str, device: torch.device):\n",
    "        \"\"\"\n",
    "        Initialize the FineTuner with base model, model path, and device.\n",
    "\n",
    "        Parameters:\n",
    "            base_model_id (str): Id of pre-trained model to use for fine-tuning.\n",
    "            model_path (str): Path to save the fine-tuned model.\n",
    "            device (torch.device): Device to run the model on.\n",
    "        \"\"\"\n",
    "        self.base_model_id = base_model_id\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.model, self.tokenizer = setup_model_and_tokenizer(base_model_id)\n",
    "\n",
    "\n",
    "    def tokenize_data(\n",
    "        self, data_points, add_eos_token=True, train_on_inputs=False, cutoff_len=512\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Tokenizes dataset of SQL related data points consisting of questions, context, and answers.\n",
    "\n",
    "        Parameters:\n",
    "            data_points (dict): A batch from the dataset containing 'question', 'context', and 'answer'.\n",
    "            add_eos_token (bool): Whether to add an EOS token at the end of each tokenized sequence.\n",
    "            cutoff_len (int): The maximum length for each tokenized sequence.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing tokenized 'input_ids', 'attention_mask', and 'labels'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            question = data_points[\"question\"]\n",
    "            context = data_points[\"context\"]\n",
    "            answer = data_points[\"answer\"]\n",
    "            if train_on_inputs:\n",
    "                user_prompt = generate_prompt_sql(question, context)\n",
    "                tokenized_user_prompt = self.tokenizer(\n",
    "                    user_prompt,\n",
    "                    truncation=True,\n",
    "                    max_length=cutoff_len,\n",
    "                    padding=False,\n",
    "                    return_tensors=None,\n",
    "                )\n",
    "                user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "                if add_eos_token:\n",
    "                    user_prompt_len -= 1\n",
    "\n",
    "            combined_text = generate_prompt_sql(question, context, answer)\n",
    "            tokenized = self.tokenizer(\n",
    "                combined_text,\n",
    "                truncation=True,\n",
    "                max_length=cutoff_len,\n",
    "                padding=False,\n",
    "                return_tensors=None,\n",
    "            )\n",
    "            if (\n",
    "                tokenized[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n",
    "                and add_eos_token\n",
    "                and len(tokenized[\"input_ids\"]) < cutoff_len\n",
    "            ):\n",
    "                tokenized[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "                tokenized[\"attention_mask\"].append(1)\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "            if train_on_inputs:\n",
    "                tokenized[\"labels\"] = [-100] * user_prompt_len + tokenized[\"labels\"][\n",
    "                    user_prompt_len:\n",
    "                ]\n",
    "\n",
    "            return tokenized\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Error in batch tokenization: {e}, Line: {e.__traceback__.tb_lineno}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "    def prepare_data(self, data, val_set_size=100) -> Dataset:\n",
    "        \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "        try:\n",
    "            train_val_split = data[\"train\"].train_test_split(\n",
    "                test_size=val_set_size, shuffle=True, seed=42\n",
    "            )\n",
    "            train_data = train_val_split[\"train\"].shuffle().map(self.tokenize_data)\n",
    "            val_data = train_val_split[\"test\"].shuffle().map(self.tokenize_data)\n",
    "            return train_data, val_data\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Error in preparing data: {e}, Line: {e.__traceback__.tb_lineno}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "    def train_model(self, train_data, val_data, training_args):\n",
    "        \"\"\"\n",
    "        Fine-tune the model with the given training and validation data.\n",
    "\n",
    "        Parameters:\n",
    "            train_data (Dataset): Training data.\n",
    "            val_data (Optional[Dataset]): Validation data.\n",
    "            training_args (TrainingArguments): Training configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model = prepare_model(self.model)\n",
    "            self.model = get_peft_model(self.model, LORA_CONFIG)\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                train_dataset=train_data,\n",
    "                eval_dataset=val_data,\n",
    "                args=training_args,\n",
    "                data_collator=DataCollatorForSeq2Seq(\n",
    "                    self.tokenizer,\n",
    "                    pad_to_multiple_of=8,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                ),\n",
    "            )\n",
    "            self.model.config.use_cache = False\n",
    "            trainer.train()\n",
    "            self.model.save_pretrained(self.model_path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in model training: {e}\")\n",
    "\n",
    "    def finetune(self, data_path, training_args):\n",
    "        \"\"\"\n",
    "        Execute the fine-tuning pipeline.\n",
    "\n",
    "        Parameters:\n",
    "            data_path (str): Path to the data for fine-tuning.\n",
    "            training_args (TrainingArguments): Training configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = load_dataset(data_path)\n",
    "            train_data, val_data = self.prepare_data(data)\n",
    "            self.train_model(train_data, val_data, training_args)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupt received, saving model...\")\n",
    "            self.model.save_pretrained(f\"{self.model_path}_interrupted\")\n",
    "            print(f\"Model saved to {self.model_path}_interrupted\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in fintuning: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acc18f09-b795-4ae3-8b0a-3beb80cfd691",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lets_finetune' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlets_finetune\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lets_finetune' is not defined"
     ]
    }
   ],
   "source": [
    "lets_finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09a0e7-7b72-401c-b997-7993b9dabdba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
